# LLM compression

The project demonstrates the work of LLM and the acceleration of inference using various optimization methods such as Graph Optimization, kv-caching.

To demonstrate the work, an application was made on Flask.
To run, you need to install the dependencies from the file requirements.txt and launch the application.

````
python -m pip install -r requirements.txt
flask --app app run
````